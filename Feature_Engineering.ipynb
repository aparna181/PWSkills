{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "NpRTY6r4YQsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "    - In machine learning, a parameter refers to a configuration variable that is internal to the model and whose value is learned from the training data. These parameters are used by the model to make predictions and are adjusted during the training process to minimize error and improve accuracy."
      ],
      "metadata": {
        "id": "fEWZtKs6YqXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation? What does negative correlation mean?\n",
        "    - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It tells us how one variable changes in relation to another. If one variable increases when the other increases, they are said to have a positive correlation. If one increases while the other decreases, they have a negative correlation. If there is no pattern, the variables have no correlation. The correlation is often measured using the Pearson correlation coefficient (r), which ranges from:\n",
        "\n",
        "       - +1: Perfect positive correlation\n",
        "       - 0: No correlation\n",
        "       - –1: Perfect negative correlation"
      ],
      "metadata": {
        "id": "C7tgFw_GaWze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "     - Machine Learning (ML) is a branch of Artificial Intelligence (AI) that allows computers to learn from data and make decisions or predictions without being explicitly programmed. It involves designing algorithms that can improve their performance over time based on experience (data).\n",
        "\n",
        "     Main Components of Machine Learning: Data, Model, Learning Algorithm, Loss Function (or Cost Function), Optimization Algorithm"
      ],
      "metadata": {
        "id": "RGNY_9bCbA2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "   - The loss value is a key indicator in machine learning that helps determine how well a model is performing. It measures the difference between the predicted output of the model and the actual (true) output from the training data.\n",
        "   \n",
        "   How it helps:\n",
        "\n",
        "   1. Indicates Prediction Accuracy: A lower loss value means the model’s predictions are close to the actual values. A higher loss value means the model is making poor predictions.\n",
        "\n",
        "   2. Guides Model Training: During training, the loss function is used to update the model’s parameters to reduce error. If the loss keeps decreasing, the model is learning correctly. If the loss is not changing or increasing, the model may be underfitting or overfitting.\n",
        "\n",
        "   3. Used for Model Comparison: When training multiple models, the one with the lowest loss on the validation data is usually considered the better model."
      ],
      "metadata": {
        "id": "FigQD-qgdNek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "    - Continuous Variables:These are numerical variables that can take any value within a range. They are measurable and often include decimals.   Examples: Height, weight, temperature, income, age.\n",
        "    - Categorical Variables: These are variables that represent categories or groups. They are not numerical and often represent labels. Examples: Gender (male/female), color (red/blue/green), city names."
      ],
      "metadata": {
        "id": "hr2G7zUde2bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "    - In machine learning, categorical variables must be converted into a numerical format because most algorithms work with numbers. This process is called encoding. Common techniques include Label Encoding, One-Hot Encoding, Ordinal Encoding, and Target Encoding, depending on the type and importance of the categorical data."
      ],
      "metadata": {
        "id": "9b7cedOrfWm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "    - Training Dataset:It is the portion of the data used to train the model. The model learns patterns, relationships, and features from this data. It includes both input features and correct outputs (labels).\n",
        "\n",
        "    - Testing Dataset: It is the portion of the data used to evaluate the model’s performance after training. The model has not seen this data before. It checks how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "yFvMMgRaioGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in the Scikit-learn library that provides functions and classes to prepare and transform data before using it in machine learning models. It is used for data preprocessing, which includes: Scaling, Normalizing, Encoding, Imputing missing values"
      ],
      "metadata": {
        "id": "3m4dGyfGj9XO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "   - A test set is a portion of the dataset that is used to evaluate the performance of a trained machine learning model. It contains unseen data (not used during training) to check how well the model generalizes to new inputs."
      ],
      "metadata": {
        "id": "y4vOhfAgklP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "    - In Python, we use Scikit-learn's train_test_split() function to split the dataset into training and testing sets.\n",
        "    - Approach to a Machine Learning Problem:\n",
        "       -  Understand the Problem: Know the objective: classification, regression, etc.\n",
        "\n",
        "      - Collect and Explore the Data: Load the dataset, check structure, and understand data types and distributions.\n",
        "      - Preprocess the Data:Handle missing values, encode categorical variables, scale features.\n",
        "      - Split the Data: Use train_test_split() to divide into training and testing sets.\n",
        "      - Select and Train a Model: Choose an appropriate algorithm (e.g., Linear Regression, Decision Tree) and train it on the training set.\n",
        "      - Evaluate the Model: Use metrics like accuracy, precision, recall, or RMSE on the test set.\n",
        "      - Tune Hyperparameters: Improve performance using techniques like Grid Search or Cross-Validation.\n",
        "      - Deploy the Model (Optional): Use the trained model in real-world applications or systems."
      ],
      "metadata": {
        "id": "1eHR8R4yk9_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "     - Exploratory Data Analysis (EDA) is the process of analyzing and visualizing data to understand its structure, patterns, and problems before building a machine learning model.Reasons to Perform EDA Before Model Fitting:\n",
        "          - Understand the Data:Identify data types, variable distributions, and relationships.\n",
        "\n",
        "          - Detect Missing or Incorrect Values:Helps in cleaning the data by handling missing, null, or outlier values.\n",
        "\n",
        "          - Reveal Patterns and Trends:EDA highlights hidden patterns and correlations that help in feature selection.\n",
        "\n",
        "          - Choose the Right Algorithms:Based on the nature of data (linear or non-linear), you can choose suitable models.\n",
        "\n",
        "          - Improve Model Performance:Well-understood and preprocessed data leads to better training and accurate predictions.\n",
        "\n",
        "          - Prevent Model Errors:Avoids issues like overfitting, underfitting, or biased predictions."
      ],
      "metadata": {
        "id": "oNueM2enmW7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "     - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables."
      ],
      "metadata": {
        "id": "NT9Ng9banKIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "    - A negative correlation means that as one variable increases, the other variable decreases, and vice versa. The two variables move in opposite directions."
      ],
      "metadata": {
        "id": "PjNbGCCvnhFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "    - In Python, you can find the correlation between variables using Pandas and Seaborn libraries. You can use .corr() to calculate the numerical correlation between variables, and Seaborn’s heatmap to visualize the strength and direction of these relationships.\n",
        "\n",
        "\n",
        "    - import pandas as pd\n",
        "\n",
        "      # Load or create a DataFrame\n",
        "        df = pd.read_csv('data.csv')  # or your dataset\n",
        "\n",
        "      # Calculate correlation matrix\n",
        "        correlation_matrix = df.corr()\n",
        "        print(correlation_matrix)"
      ],
      "metadata": {
        "id": "smPJ2LmKoawX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "    - Causation (also called causality) means that one event directly causes another to happen. In other words, a change in one variable leads to a change in another.\n",
        "    -  Difference:\n",
        "          - Correlation refers to a statistical relationship between two variables, where changes in one variable are associated with changes in the other. However, this does not mean that one variable causes the other to change.\n",
        "          - causation implies a cause-and-effect relationship, where one variable directly influences or leads to a change in another variable.\n",
        "          -  For example, there may be a correlation between ice cream sales and drowning cases during summer, but eating ice cream does not cause drowning — both are related to a third factor, which is hot weather."
      ],
      "metadata": {
        "id": "keToUQr3dRCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "    - An optimizer is a key component in machine learning and deep learning that is used to adjust the model’s parameters (like weights and biases) to minimize the loss function and improve the model’s accuracy. During training, optimizers help the model learn by updating the weights in the direction that reduces prediction errors.\n",
        "    - Common Types of Optimizers:\n",
        "         - Gradient Descent (GD): It calculates the gradient of the loss function and updates the parameters in the opposite direction of the gradient to minimize loss. It works well for small datasets. Example:\n",
        "Used in Linear Regression to minimize Mean Squared Error.\n",
        "\n",
        "         - Stochastic Gradient Descent (SGD): Unlike GD, which uses the whole dataset, SGD uses one data point at a time to update the weights.It is faster and more suitable for large datasets, but more noisy.Example: Used in online learning models or when data is streamed in batches.\n",
        "         -  Mini-Batch Gradient Descent: A mix of GD and SGD, where the model is trained using small batches of data. Balances speed and stability.Example: Commonly used in training deep learning models like CNNs or RNNs.\n",
        "         - Adam (Adaptive Moment Estimation): Combines ideas from Momentum and RMSProp optimizers. It adapts the learning rate for each parameter using estimates of the first and second moments (mean and variance of gradients). Widely used in deep learning due to fast convergence.Example: Used in training deep neural networks, e.g., image classification using TensorFlow/Keras.\n",
        "         - RMSProp (Root Mean Square Propagation): Adjusts the learning rate based on a moving average of recent gradients. Works well for recurrent neural networks and non-stationary problems. Example: Used in time-series forecasting models.\n",
        "         - 6. Adagrad: Assigns individual learning rates to each parameter and adapts them based on frequency of updates. Useful for sparse data like text classification. Example: Used in Natural Language Processing (NLP) tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "ATVHtabdepCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "    - sklearn.linear_model is a module in the Scikit-learn library that provides various linear models for regression and classification tasks in machine learning. This module includes models that assume a linear relationship between the input features (X) and the target variable (y)."
      ],
      "metadata": {
        "id": "kQ1VwkBTgEzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "    - The model.fit() function in machine learning is used to train a model on the given data. It tells the model to learn the patterns from the training dataset by adjusting its internal parameters.\n",
        "    - It takes input features (X) and target labels (y). It uses them to optimize the model by minimizing the loss function. After calling fit(), the model is ready to make predictions using predict().\n",
        "    - syntax: model.fit(X, y)\n",
        "        \n",
        "        - X → Input features (independent variables), usually in the form of a NumPy array or pandas DataFrame.\n",
        "\n",
        "        - y → Target variable (dependent variable), usually a 1D array or Series.\n",
        "        - model.fit(X, y) is the training step where the model learns from the data. Both X and y are essential inputs: X for features and y for labels or outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "W4haorKvgbnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do?What arguments must be given?\n",
        "    - The model.predict() function is used to make predictions on new or unseen data after the model has been trained using model.fit(). It takes the input features (X) and returns the predicted output (y) based on what the model has learned during training.\n",
        "    - The model.predict() function requires only one main argument: X → The input features (new data) you want the model to make predictions on. It must be in the same format and shape as the data used during training.Usually passed as a NumPy array, Pandas DataFrame, or list of feature values."
      ],
      "metadata": {
        "id": "FrE9z8b2hqq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "    - Continuous Variables:These are numerical variables that can take any value within a range. They are measurable and often include decimals.   Examples: Height, weight, temperature, income, age.\n",
        "    - Categorical Variables: These are variables that represent categories or groups. They are not numerical and often represent labels. Examples: Gender (male/female), color (red/blue/green), city names."
      ],
      "metadata": {
        "id": "AnbI5xVIiWhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    - Feature scaling is a data preprocessing technique used to normalize or standardize the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model’s performance by bringing them to a similar scale.\n",
        "    - Feature scaling helps in machine learning by ensuring that all input features have similar ranges or scales, which improves the efficiency and performance of many algorithms."
      ],
      "metadata": {
        "id": "Cs8TQlcziiDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "    - In Python, feature scaling is commonly performed using the sklearn.preprocessing module from the Scikit-learn library. Two widely used scaling methods are Standardization and Min-Max Scaling."
      ],
      "metadata": {
        "id": "FEnfReJJkCcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in the Scikit-learn library that provides a set of tools for preprocessing data before using it in machine learning models.  sklearn.preprocessing provides essential tools to transform raw data into a usable format, helping machine learning models learn better and perform more accurately."
      ],
      "metadata": {
        "id": "BTOKmcotlHvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "     - In Python, we use the train_test_split() function from the sklearn.model_selection module to split data into training and testing sets. This helps to train the model on one portion of the data and evaluate its performance on unseen data.\n",
        "     - Using train_test_split() helps us divide the dataset so we can train the model on one part and test its accuracy on the other, ensuring that the model performs well on new, unseen data."
      ],
      "metadata": {
        "id": "4-OUJCMWlmTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "    - Data encoding is the process of converting categorical (non-numeric) data into numerical form so that it can be used by machine learning algorithms. Most models can only work with numbers, so encoding is essential when dealing with features like gender, color, city, etc.\n",
        "    - It is needed :Machine learning models cannot process text or labels directly. Encoding helps models understand and learn from categorical data."
      ],
      "metadata": {
        "id": "cBZRL8OmmFZJ"
      }
    }
  ]
}