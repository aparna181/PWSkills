{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree - Assignment"
      ],
      "metadata": {
        "id": "-p4fhtgQkfB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "   - A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It mimics human decision-making by breaking down a dataset into smaller and smaller subsets while simultaneously developing an associated decision tree structure.\n",
        "   - How it works for Classification:\n",
        "       - Select the best attribute using a metric like: Gini Index , Information Gain (based on Entropy), Gain Ratio\n",
        "\n",
        "       - Split the dataset into subsets based on the selected attribute.\n",
        "\n",
        "       - Repeat the process recursively for each child node: Continue splitting until a stopping condition is met (e.g., all samples at a node have the same label, or a maximum depth is reached).\n",
        "\n",
        "       - Assign a class label to each leaf node based on the majority class of samples in that node.\n",
        "\n"
      ],
      "metadata": {
        "id": "ElgljpXcklbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.How   do they impact the splits in a Decision Tree?\n",
        "     - In decision trees, impurity measures are used to determine how mixed the classes are within a node. Two commonly used impurity measures are Gini Impurity and Entropy.\n",
        "     - Gini Impurity: Gini Impurity measures the probability of misclassifying a randomly chosen element from the dataset if it were labeled randomly based on the distribution of labels in the subset.\n",
        "     - Entropy: Entropy measures the level of disorder or impurity in the node. It's rooted in information theory and quantifies the expected amount of information (bits) needed to classify a sample.\n",
        "     - Impact on Decision Tree Splits:\n",
        "          - During tree construction, the algorithm evaluates each feature's potential to split the data.\n",
        "          - It chooses the split that results in the largest reduction in impurity. For Gini, it picks the split with the lowest Gini Impurity. For Entropy, it picks the split with the highest Information Gain (i.e., biggest reduction in Entropy).\n",
        "         "
      ],
      "metadata": {
        "id": "Iicc7w20lXlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "    - Pre-Pruning and Post-Pruning are techniques used to prevent overfitting in decision trees by controlling the growth of the tree.\n",
        "    - Pre-pruning, also known as early stopping, involves halting the growth of the tree during its construction phase based on certain conditions such as maximum depth, minimum number of samples required to split a node, or a minimum gain in impurity. This helps in creating a simpler model and significantly reduces training time, which is a practical advantage.\n",
        "    -  post-pruning involves allowing the tree to grow fully and then removing branches that do not improve the model’s performance on a validation dataset. A key advantage of post-pruning is that it often improves generalization by eliminating branches that lead to overfitting, thus making the model more robust on unseen data.\n",
        "    -  The main difference between the two lies in the timing—pre-pruning stops tree growth during training, whereas post-pruning simplifies the tree after it is fully grown.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JjI4PaMqoHB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "     - Information Gain is a metric used in decision trees to measure the effectiveness of an attribute in classifying the training data.\n",
        "     - It quantifies the reduction in entropy, or uncertainty, after a dataset is split on a particular feature. In simpler terms, it tells us how much \"information\" a feature gives us about the class labels. The higher the information gain, the more effectively that feature separates the data into distinct classes. During the tree-building process, the algorithm evaluates all possible splits and selects the one with the highest information gain as the best split.\n",
        "     - This is important because it ensures that each decision in the tree contributes maximally to reducing disorder and helps in building a more accurate and efficient model."
      ],
      "metadata": {
        "id": "RJ1QwoczpPhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "    - Decision Trees are widely used in real-world applications due to their simplicity and interpretability. Common applications include medical diagnosis, where trees help predict diseases based on symptoms; credit scoring and loan approval, where financial institutions use them to assess the risk level of applicants; fraud detection, where patterns of fraudulent activity are identified; customer segmentation in marketing; and predictive maintenance in industrial systems.\n",
        "    - The main advantages of decision trees are that they are easy to understand and visualize, require little data preprocessing, and can handle both numerical and categorical data. However, they also have some limitations. Decision trees are prone to overfitting, especially with complex datasets, and small changes in data can result in a completely different tree, making them unstable. Additionally, they may struggle with imbalanced datasets and tend to prefer features with more levels. Despite these limitations, decision trees remain a powerful and popular tool in machine learning and data analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "xxfI6aEMrC6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "     -  Load the Iris Dataset\n",
        "     - Train a Decision Tree Classifier using the Gini criterion\n",
        "     - Print the model’s accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "4GPjmORDrrkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLHTzDMfsN-p",
        "outputId": "ceb443a0-ad1a-4ac5-a50f-32c21845f25d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "     - Load the Iris Dataset\n",
        "     - Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "2bXRb1vgswOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "pred_limited = tree_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree (no depth limit)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, pred_full)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy with max_depth=3: {acc_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {acc_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW51W1wvtAw3",
        "outputId": "0e30f1be-7d28-4fba-8349-4dfa8b9be176"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor.\n",
        "- Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "DLsYVU4ItkEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiBjOUEQkZ1x",
        "outputId": "772d4c48-6490-493f-c219-51c2740724bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate Mean Squared Error\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "ETgC5T0FuBs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and predict\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xgc2BxKvxSb",
        "outputId": "0a14d0f0-b88c-4607-a3ce-3be06e873ac0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Ans:\n",
        "- To build a predictive model for disease detection, the first step is to handle missing values. For numerical features, I would use imputation techniques such as filling with the mean or median, while for categorical features, I would fill missing values with the mode or use a placeholder like \"Unknown\".\n",
        "- The next step is to encode categorical variables so that they can be used by the machine learning model. For Decision Trees, label encoding works well, but one-hot encoding can also be used for nominal categories.\n",
        "-  After preprocessing, I would split the dataset into training and testing sets and train a Decision Tree Classifier using the processed data.\n",
        "-  To optimize performance, I would apply GridSearchCV or RandomizedSearchCV to tune hyperparameters such as max_depth, min_samples_split, and criterion.\n",
        "- Once the model is tuned, I would evaluate its performance using metrics like accuracy, precision, recall, F1-score, and ROC-AUC, especially considering the medical context where false negatives can be critical.\n",
        "- Finally, the business value of this model lies in its ability to assist doctors in early disease detection, support faster decision-making, reduce manual errors, and ultimately improve patient outcomes while optimizing resource allocation in the healthcare system.\n",
        "\n"
      ],
      "metadata": {
        "id": "PsQGf1g_v8IA"
      }
    }
  ]
}