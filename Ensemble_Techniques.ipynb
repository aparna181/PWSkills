{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "    - Ensemble learning is a machine learning approach where we do not rely on a single model to make predictions. Instead, we build several different models and combine their outputs to reach a final decision. The main idea is that different models may capture different patterns in the data, and when their results are combined, the overall prediction becomes more reliable and accurate.\n",
        "    - Think of it like a group decision-making process. If only one person gives an answer, it might be wrong due to their limited knowledge. But if several people with different experiences give their answers and we take a majority vote or average their suggestions, the final answer is usually better.\n",
        "    - Ensemble learning reduces the risk of errors that a single model might make. It also helps in improving stability—meaning small changes in the training data are less likely to affect the final prediction. Common ways to build ensembles include:\n",
        "       - Bagging, where models are trained in parallel on different random subsets of data (e.g., Random Forest).\n",
        "       - Boosting, where models are trained one after another, each trying to fix the mistakes of the previous model (e.g., AdaBoost, XGBoost)\n",
        "       \n",
        "       - Stacking, where outputs from different models are combined using another model to make the final prediction.\n"
      ],
      "metadata": {
        "id": "GQFvmPR6kf14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        "   - Bagging and Boosting are both techniques used in ensemble learning to improve the performance of machine learning models, but they work in different ways. Bagging, which stands for Bootstrap Aggregating, trains multiple models independently and in parallel by using different random subsets of the training data. Each model has an equal say when making the final prediction, and this helps reduce variance and avoid overfitting.\n",
        "   - On the other hand, Boosting trains models one after another in a sequence, where each new model tries to fix the errors made by the previous ones. Boosting focuses more on difficult cases by giving them higher importance, which helps reduce bias and improve overall accuracy. Popular examples of Bagging include Random Forest, while Boosting includes algorithms like AdaBoost and Gradient Boosting. In summary, Bagging combines models by averaging to make predictions more stable, whereas Boosting builds models sequentially to correct mistakes and enhance accuracy."
      ],
      "metadata": {
        "id": "HQAjUN9dl7Q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "    - Bootstrap sampling is a technique where we create multiple new datasets by randomly selecting samples with replacement from the original training data. This means some data points may appear more than once in a new sample, while others might not appear at all. In Bagging methods like Random Forest, bootstrap sampling is used to generate different training sets for each individual model (usually decision trees). Because each model sees a slightly different version of the data, they learn different patterns, which makes the models diverse. This diversity is important because when the predictions from all these models are combined (by voting or averaging), the overall result becomes more accurate and less likely to overfit the training data. So, bootstrap sampling helps Bagging methods improve stability and performance by ensuring that each model is trained on a unique subset of data."
      ],
      "metadata": {
        "id": "9mUvfe8LmckC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "    - Out-of-Bag (OOB) samples are the data points that are not selected when creating a bootstrap sample for training an individual model in ensemble methods like Random Forest. Since bootstrap sampling is done with replacement, about one-third of the original data is usually left out (not included) in each sample. These left-out samples are called OOB samples. They act like a built-in test set for that particular model because the model has never seen them during training. The OOB score is calculated by using these OOB samples to test the model’s predictions without needing a separate validation dataset. By averaging the prediction accuracy on OOB samples across all the models in the ensemble, we get an estimate of how well the entire ensemble will perform on unseen data. This makes OOB scoring a convenient and efficient way to evaluate the model’s performance during training.\n"
      ],
      "metadata": {
        "id": "hWPTpwWDmo_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "      - Feature importance in a single Decision Tree shows how much each feature contributes to reducing uncertainty (or impurity) when splitting the data at different nodes. It reflects the importance based only on one tree’s structure, which can be sensitive to noise and may overfit the training data. In contrast, Random Forest calculates feature importance by averaging the importance scores across many trees built on different bootstrap samples and random subsets of features. This averaging process makes the feature importance in Random Forest more stable and reliable, as it reduces the bias and variance that can occur in a single tree. Therefore, Random Forest provides a better overall estimate of which features are truly important for predicting the target."
      ],
      "metadata": {
        "id": "cC1UcbRlm4YP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "  - Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "  -  Train a Random Forest Classifier\n",
        "  - Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "vQZqe7tqnIOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for easy sorting and display\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance descending and get top 5\n",
        "top5 = feat_imp_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 important features:\")\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LdfKJAhnA8M",
        "outputId": "070514f4-314e-4eff-81a4-d388b722818d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 important features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to:\n",
        "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "wW9E-zTMnfYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # <-- updated here\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "acc_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy of single Decision Tree: {acc_dt:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {acc_bagging:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T6hFV5jn1MZ",
        "outputId": "b8eabe3f-8127-4df7-ff46-564fbb0bde79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "- Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "RCScK4r_n46N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'n_estimators': [10, 50, 100, 200]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit GridSearch to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict on test data using best estimator\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best parameters found:\", best_params)\n",
        "print(f\"Final accuracy on test set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRp3xtkUn_4g",
        "outputId": "29d7fffb-86be-41a9-c730-0afb7b98f1d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best parameters found: {'max_depth': 3, 'n_estimators': 200}\n",
            "Final accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to:\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "BKxDZv3hoJS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Bagging Regressor with Decision Tree as base estimator\n",
        "bagging_regressor = BaggingRegressor(random_state=42, n_estimators=50)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_regressor.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "random_forest_regressor = RandomForestRegressor(random_state=42, n_estimators=50)\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest_regressor.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the Mean Squared Errors for comparison\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {mse_bagging:.4f}\")\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x27uIvsXoQDO",
        "outputId": "980e17be-8254-43e5-bfa3-cbb519c29f41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2579\n",
            "Mean Squared Error of Random Forest Regressor: 0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "qoJRYugjobjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Choosing between Bagging and Boosting\n",
        "Since the goal is to predict loan default accurately, the choice depends on the data and the problem characteristics. If the dataset is large with noisy data or has high variance (models easily overfit), Bagging (like Random Forest) is a good choice because it reduces variance by averaging many independent models trained on different samples. If the problem requires improving accuracy by reducing bias and handling complex patterns, Boosting (like AdaBoost, XGBoost) is better because it builds models sequentially, focusing on correcting previous errors. Often, I would start with both and compare performance to decide.\n",
        "\n",
        "Step 2: Handling Overfitting\n",
        "Overfitting happens when the model learns noise instead of true patterns. To prevent it, I would use techniques like:\n",
        "\n",
        "Setting limits on tree depth or number of leaves for base models.\n",
        "\n",
        "Using regularization parameters (e.g., learning rate in boosting).\n",
        "\n",
        "Applying early stopping when the validation error stops improving.\n",
        "\n",
        "Using cross-validation to tune hyperparameters carefully.\n",
        "\n",
        "For Bagging, ensuring enough diversity among base learners by sampling data/features randomly.\n",
        "\n",
        "Step 3: Selecting Base Models\n",
        "Decision Trees are the most common base models for both Bagging and Boosting because they handle non-linear relationships and categorical variables well. For Bagging, using fully grown trees helps reduce bias, while for Boosting, using shallow trees (weak learners) prevents overfitting and helps incremental learning. I might also try other models like logistic regression or SVM, but trees are typically more flexible for this kind of task.\n",
        "\n",
        "Step 4: Evaluating Performance Using Cross-Validation\n",
        "I would use k-fold cross-validation to split data into training and validation sets multiple times to ensure the model generalizes well and avoid overfitting. This helps assess model stability and performance across different samples. Metrics like ROC-AUC, precision, recall, and F1-score are important for loan default prediction since class imbalance and false negatives are critical issues. Cross-validation also helps in hyperparameter tuning.\n",
        "\n",
        "Step 5: How Ensemble Learning Improves Decision-Making in This Context\n",
        "Ensemble methods combine multiple models to create a stronger predictor, which leads to more accurate and robust loan default predictions. This means the financial institution can better identify risky customers, reduce bad loans, and optimize lending decisions. By reducing errors and overfitting, ensembles increase confidence in predictions, helping stakeholders make informed, data-driven choices. Ultimately, this reduces financial losses and supports sustainable business growt"
      ],
      "metadata": {
        "id": "vcDw4qHVpDV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Create a synthetic dataset to simulate loan default data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_classes=2, weights=[0.7, 0.3],\n",
        "                           random_state=42)\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Define Bagging (Random Forest) and Boosting (AdaBoost) models\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),  # Updated from base_estimator to estimator\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define hyperparameter grids to handle overfitting and tune models\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "ada_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# 4. Use GridSearchCV for hyperparameter tuning with 5-fold cross-validation\n",
        "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "ada_grid = GridSearchCV(ada, ada_param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "\n",
        "# Fit models\n",
        "rf_grid.fit(X_train, y_train)\n",
        "ada_grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Random Forest params:\", rf_grid.best_params_)\n",
        "print(\"Best AdaBoost params:\", ada_grid.best_params_)\n",
        "\n",
        "# 5. Evaluate on test set using ROC-AUC\n",
        "rf_pred_proba = rf_grid.best_estimator_.predict_proba(X_test)[:, 1]\n",
        "ada_pred_proba = ada_grid.best_estimator_.predict_proba(X_test)[:, 1]\n",
        "\n",
        "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
        "ada_auc = roc_auc_score(y_test, ada_pred_proba)\n",
        "\n",
        "print(f\"Random Forest ROC-AUC on test set: {rf_auc:.4f}\")\n",
        "print(f\"AdaBoost ROC-AUC on test set: {ada_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHPlbazAoqSc",
        "outputId": "06fdc86b-be5d-4bb6-a28b-5806705182d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best AdaBoost params: {'learning_rate': 1, 'n_estimators': 100}\n",
            "Random Forest ROC-AUC on test set: 0.9781\n",
            "AdaBoost ROC-AUC on test set: 0.9237\n"
          ]
        }
      ]
    }
  ]
}